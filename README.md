# AWS-Data---Engineering-Project-
1️⃣ Data Ingestion: 📊
             What I Did: Started with a Spotify dataset from Kaggle, split it into
             3 CSV files, and uploaded them to S3. 

2️⃣ Data Transformation: 🔄
             What I Did: Created a visual ETL pipeline in AWS Glue to clean and
             transform the data. 
Optimizations:🚀
          Partitioning Data: Enhanced query performance.
          Efficient Formats: Switched to Parquet for speedier operations.
          Glue Job Tuning: Adjusted configurations to optimize performance 
          and cost.
          Script Efficiency: Refined PySpark code for better results.

🌟 Achievements:
             30% Reduction in data processing time during transformation! 
             Achieved this by optimizing Glue scripts and applying best practices.

3️⃣ Data Storage: 🏗️
             What I Did: Stored the processed data back in S3, serving as my data
             warehouse. 

4️⃣ Data Analysis: 📈
            What I Did: Queried the processed data using AWS Athena. 
            Discover Queries: Check out this example query and its results!
            Attaching a snapshot of the Athena query editor with sample results 
            below.
